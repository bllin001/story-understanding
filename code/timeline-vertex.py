# # -*- coding: utf-8 -*-
# """bllin001 (Nov 24, 2025, 5:55:05‚ÄØPM)

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/embedded/projects/instr-cs795-fall25-hqin-1/locations/us-central1/repositories/60abc5a2-68df-4f71-bfdb-12d2b7aca20c
# """

# # Search for 'story-understanding.zip' in the current directory and its subdirectories
# !find . -name "story-understanding.zip"

# # Commented out IPython magic to ensure Python compatibility.
# # %cd ./content

# print('Current working directory:')
# !pwd

# print('Contents of the current directory:')
# !ls -F

# # prompt: unzip story-understanding file without show many results

# !unzip -q story-understanding.zip

# # Commented out IPython magic to ensure Python compatibility.
# # prompt: Set story-understanding as the main directory

# # %cd story-understanding

# ============================
# üìå TIMELINE GENERATOR (Vertex AI - Gemini 2.0 Flash)
# ============================

import os, re, json, time, datetime, random, csv
from tqdm.notebook import tqdm
from vertexai import init
from vertexai.generative_models import GenerativeModel

# ============================
# ‚öôÔ∏è CONFIG
# ============================
PROJECT_ID = "instr-cs795-fall25-hqin-1"   # ‚Üê TU PROYECTO
LOCATION   = "us-central1"
MODEL_NAME = "gemini-2.0-flash"
COLLECTION = "mozambique"  # cambia entre drc, burundi, sudan, mozambique

DATA_PATH   = f"./data/collections/{COLLECTION}_articles_formatted.txt"
PROMPT_PATH = "./prompt/prompt-timeline.txt"

OUT_TXT       = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_timeline_raw.txt"
OUT_JSON      = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_timeline.json"
OUT_COST_TXT  = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_timeline_costs.txt"
OUT_COST_CSV  = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_timeline_costs.csv"

# ============================
# üí∞ COST CONFIG (Gemini 2.0 Flash)
# ============================
COST_INPUT  = 0.15 / 1_000_000    # USD per token
COST_OUTPUT = 0.30 / 1_000_000    # USD per token

# ============================
# üöÄ INIT MODEL
# ============================
init(project=PROJECT_ID, location=LOCATION)
model = GenerativeModel(MODEL_NAME)

# ============================
# üìä GLOBAL COST TRACKING
# ============================
total_input_tokens  = 0
total_output_tokens = 0
total_cost_usd      = 0.0

# fila por art√≠culo
cost_rows = []  # {"article_index","title","input_tokens","output_tokens","cost_usd","time_sec"}

# ============================
# üìå LOAD PROMPT
# ============================
with open(PROMPT_PATH, "r", encoding="utf-8") as f:
    BASE_PROMPT = f.read()

def build_prompt(text, pub_date):
    return (BASE_PROMPT
            .replace("{narrative_text}", text)
            .replace("{PublicationDate}", pub_date))

# ============================
# üîÅ SAFE MODEL INVOKER + COST + TIME
# ============================
def ask_gemini(prompt, index=None, title=None, retries=5):
    global total_input_tokens, total_output_tokens, total_cost_usd, cost_rows

    for attempt in range(retries):
        try:
            t0 = time.time()
            res = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.0,
                    "max_output_tokens": 8192
                }
            )
            elapsed = time.time() - t0

            text = res.text

            # ---- Token usage ----
            usage = getattr(res, "usage_metadata", None)
            if usage is not None:
                input_tok  = getattr(usage, "input_token_count", 0) or getattr(usage, "prompt_token_count", 0)
                output_tok = getattr(usage, "output_token_count", 0) or getattr(usage, "candidates_token_count", 0)
            else:
                input_tok = output_tok = 0

            # ---- Cost calc ----
            cost_total = (input_tok / 1_000_000) * COST_INPUT + (output_tok / 1_000_000) * COST_OUTPUT
            total_input_tokens  += input_tok
            total_output_tokens += output_tok
            total_cost_usd      += cost_total

            # Save per article
            cost_rows.append({
                "article_index": index,
                "title": title,
                "input_tokens": input_tok,
                "output_tokens": output_tok,
                "cost_usd": round(cost_total, 8),
                "time_sec": round(elapsed, 4),
            })

            return text

        except Exception as e:
            print(f"‚ö†Ô∏è Gemini error (attempt {attempt+1}/{retries}): {e}")
            time.sleep(1.5 * (attempt + 1))

    raise RuntimeError("‚ùå MAX RETRIES REACHED")


# ============================
# üîé JSON PARSER
# ============================
def parse_json(output):
    try:
        return json.loads(output)
    except:
        block = re.search(r"```json\s*([\s\S]*?)```", output)
        if block:
            try: return json.loads(block.group(1))
            except: pass
        try: return json.loads(output.strip("` \n"))
        except:
            print("‚ùå BAD JSON OUTPUT (first 300 chars):\n", output[:300])
            return {}

# ============================
# üß† PROCESS ONE ARTICLE
# ============================
def process_article(article, pub_date, meta):
    response = ask_gemini(
        build_prompt(article, pub_date),
        index=meta.get("index"),
        title=meta.get("title")
    )
    parsed = parse_json(response)
    if isinstance(parsed, list) and len(parsed)==1:
        parsed = parsed[0]

    # metadata injection
    parsed["title"]           = meta.get("title","")
    parsed["url"]             = meta.get("url","")
    parsed["source"]          = meta.get("source","")
    parsed["section"]         = meta.get("section","")
    parsed["PublicationDate"] = pub_date

    return parsed, response

# ============================
# üìÑ LOAD COLLECTION
# ============================
with open(DATA_PATH, "r", encoding="utf-8") as f:
    text_all = f.read()

articles = [a.strip() for a in re.split(r"\n={3,}\n", text_all) if a.strip()]

# ============================
# üì¶ LOAD EXISTING CHECKPOINTS
# ============================
os.makedirs(os.path.dirname(OUT_JSON), exist_ok=True)

structured_all = json.load(open(OUT_JSON,"r")) if os.path.exists(OUT_JSON) else []
existing_raw   = open(OUT_TXT,"r").read() if os.path.exists(OUT_TXT) else ""

done = set((x.get("title"), x.get("PublicationDate")) for x in structured_all)

# ============================
# üöß RUN EXTRACTION
# ============================
new_structured = []
raw_to_append  = []

for idx, art in enumerate(tqdm(articles, desc=f"üì∞ Extracting {COLLECTION}"), start=1):

    title = re.search(r"^Title:\s*(.*)", art, re.M)
    title = title.group(1).strip() if title else f"Article {idx}"

    pub = re.search(r"^Publication Date:\s*(.+)", art, re.M)
    pub = pub.group(1).strip() if pub else "Unknown"
    pub_iso = re.search(r"\d{4}-\d{2}-\d{2}", pub)
    pub = pub_iso.group(0) if pub_iso else pub

    if (title, pub) in done:
        print(f"‚è≠Ô∏è Already processed: {title}")
        continue

    meta = {
        "index": idx, "title": title,
        "url": re.search(r"^URL:\s*(.*)", art, re.M).group(1).strip() if re.search(r"^URL:", art, re.M) else "",
        "source": re.search(r"^Source:\s*(.*)", art, re.M).group(1).strip() if re.search(r"^Source:", art, re.M) else "",
        "section": re.search(r"^Section:\s*(.*)", art, re.M).group(1).strip() if re.search(r"^Section:", art, re.M) else "",
    }

    result, raw = process_article(art, pub, meta)
    new_structured.append(result); structured_all.append(result)
    raw_to_append.append(f"\n=== {title} ===\n{raw}\n")

    if len(new_structured) % 3 == 0:
        open(OUT_TXT,"a").write("\n".join(raw_to_append))
        json.dump(structured_all, open(OUT_JSON,"w"), indent=2, ensure_ascii=False)
        raw_to_append = []
        print("üíæ Checkpoint Saved")

# FINAL SAVE
if raw_to_append: open(OUT_TXT,"a").write("\n".join(raw_to_append))
json.dump(structured_all, open(OUT_JSON,"w"), indent=2, ensure_ascii=False)
print("\nüéâ DONE! Timeline saved at:", OUT_JSON)

# ============================
# üíæ SAVE COST STATS
# ============================
with open(OUT_COST_TXT,"w",encoding="utf-8") as f:
    f.write(f"Model: {MODEL_NAME}\nCollection: {COLLECTION}\nArticles: {len(cost_rows)}\n\n")
    f.write(f"Total input tokens: {total_input_tokens}\nTotal output tokens: {total_output_tokens}\nTotal cost: ${total_cost_usd:.6f}\n\n")
    for r in cost_rows:
        f.write(f"- idx={r['article_index']} | {r['title']} | in={r['input_tokens']} | out={r['output_tokens']} | time={r['time_sec']}s | cost=${r['cost_usd']}\n")

with open(OUT_COST_CSV,"w",newline="",encoding="utf-8") as f:
    w=csv.writer(f); w.writerow(["idx","title","input","output","cost","time_sec"])
    for r in cost_rows: w.writerow([r["article_index"],r["title"],r["input_tokens"],r["output_tokens"],r["cost_usd"],r["time_sec"]])

print("üí∞ Cost info saved at:", OUT_COST_TXT, "and", OUT_COST_CSV)

import json
from datetime import datetime

# ========= CONFIG =========
INPUT_FILE  = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_timeline.json"                 # Gemini
OUTPUT_FILE = f"./output/timelines/{COLLECTION}/{MODEL_NAME}/{COLLECTION}_narrative_timeline.json"       # Normalized GPT-like


# ========= DATE UTILS =========
def clean_date(date_str):
    """Normalize to YYYY-MM-DD; return None if invalid."""
    if not date_str or date_str in ["", "null", None]:
        return None

    # Remove extra quotes/spaces
    date_str = str(date_str).strip().replace('"', '')

    # Case: "2024-11-20T15:00:00Z"
    if "T" in date_str:
        try:
            return datetime.strptime(date_str[:10], "%Y-%m-%d").strftime("%Y-%m-%d")
        except:
            pass

    # Case: "2024-9-7", "2024/11/20"
    for fmt in ["%Y-%m-%d", "%Y/%m/%d", "%Y-%m-%dT%H:%M"]:
        try:
            return datetime.strptime(date_str, fmt).strftime("%Y-%m-%d")
        except:
            pass

    return None


def safe_sort_key(ev):
    """Put valid dates first, then invalid ones; sort ascending."""
    d = ev.get("Date")
    if not d:
        return (1, "")  # invalid goes to the end
    return (0, d)      # valid sorted normally


def normalize_events(event_list):
    """Clean dates and return sorted list."""
    normalized = []
    for ev in event_list:
        ev["Date"] = clean_date(ev.get("Date"))
        normalized.append(ev)

    # Sort: valid dates first, undefined afterward
    normalized.sort(key=safe_sort_key)
    return normalized


# ========= MAIN CONVERSION =========
def convert_gemini_to_gpt():
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    final_output = []

    for article in data:

        # Normalize metadata fields
        output = {
            "title":  article.get("title", ""),
            "url":    article.get("url", ""),
            "source": article.get("source", ""),
            "section": article.get("section", ""),
            "PublicationDate": clean_date(article.get("PublicationDate"))
        }

        # Normalize + Sort events
        events = normalize_events(article.get("Events", []))

        # Add GPT-style fields
        output["event_count"] = len(events)
        output["timeline"]    = events

        final_output.append(output)

    # Save normalized file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

    print("üéâ Normalized + Sorted timeline exported ‚Üí", OUTPUT_FILE)


# ========= RUN =========
convert_gemini_to_gpt()

# # prompt: Zip the directory where the ouput was saved: Was output/timelines/collection_name/model_name

# !zip -r output/timelines/{COLLECTION}/{MODEL_NAME}.zip output/timelines/{COLLECTION}/{MODEL_NAME}