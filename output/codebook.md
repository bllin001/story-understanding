ğŸ“˜ Event Extraction Evaluation Codebook

Manual Assessment of News Event Timelines Generated by LLMs

(Applicable to Gemini, GPT, or similar models)

â¸»

ğŸ§© Purpose of This Codebook

This document provides guidelines for evaluating the quality of events extracted from news articles by Large Language Models (LLMs). The evaluator must verify whether:
	â€¢	The assigned date is accurate.
	â€¢	The root event (main trigger of the narrative) is correctly identified.
	â€¢	The event type is appropriate.
	â€¢	The event description is not ambiguous.
	â€¢	The event is relevant to the articleâ€™s core narrative.

Each event is rated according to five dimensions, described below.

â¸»

ğŸ“Œ Evaluation Dimensions

â¸»

1ï¸âƒ£ Date Correctness â€” Eval_DateCorrect (0â€“1)

Assesses whether the model assigned the correct event date, based on explicit or implicit textual evidence.

Scoring Criteria

Value	Meaning
1	The date is correct and properly inferred (explicit or implicit).
0	The date is incorrect, invented, or not supported by the text.

ğŸ’¡ Valid inference examples:
â€œyesterdayâ€, â€œlast weekâ€, â€œon Mondayâ€â€¦ as long as they are correctly calculated from the publication date.

â¸»

2ï¸âƒ£ Root Event Identification â€” Eval_RootEvent (0â€“1)

Determines whether the event marked as the origin or main event of the article is indeed the central trigger.

Scoring Criteria

Value	Meaning
1	It is the central event that drives the rest of the narrative.
0	It is secondary, contextual, or a consequenceâ€”not the root cause.

ğŸ’¡ Tip: The root event usually answers:
â€œWhat happened that caused this news story to exist?â€

â¸»

3ï¸âƒ£ Event Type Accuracy â€” Eval_EventType (0â€“1)

Checks whether the assigned event type (e.g., killing, protest, election, announcement, arson) accurately reflects the described action.

Scoring Criteria

Value	Meaning
1	Event type clearly matches the action described.
0	Incorrect, too generic, inconsistent, or misleading.

ğŸ’¡ Example:
If the article says â€œmurdered,â€ the type should be killing, not crime.

â¸»

4ï¸âƒ£ Ambiguity Level â€” Eval_EventAmbiguity (1â€“3)

Evaluates the clarity and specificity of the event description.

Score	Definition
1 = High Ambiguity	Vague, missing key actors or actions. Hard to understand.
2 = Moderate Ambiguity	Understandable but lacks detail or precision.
3 = Low Ambiguity	Clear, well-defined, specific, and informative.


â¸»

5ï¸âƒ£ Relevance to Article Narrative â€” Eval_Relevance (1â€“3)

Measures how important the event is to the meaning and understanding of the news story.

Score	Definition
1 = Low Relevance	Could be removed without affecting understanding.
2 = Medium Relevance	Related, but not central; adds context.
3 = High Relevance	Crucial; removing it would break the narrative.

ğŸ’¡ Note: Relevance is narrative-based, not moral.
A horrific act may still be irrelevant to the main story.

â¸»

ğŸ§® Final Score Computation (Event Quality Score â€” EQS)

Each event receives an EQS between 0 and 1.

ğŸ”¢ Recommended weighted formula (prioritizes chronology):

EQS = (
  2*DateCorrect +
  1.5*RootEvent +
  1*EventType +
  0.75*AmbiguityNorm +
  0.75*RelevanceNorm
) / 6

ğŸ“Œ Normalization

AmbiguityNorm = (Eval_EventAmbiguity - 1) / 2
RelevanceNorm = (Eval_Relevance - 1) / 2


â¸»

ğŸ“ Evaluator Comment Field

Use this to record qualitative observations, e.g.:
	â€¢	Incorrect inferred date.
	â€¢	Missing actors causing ambiguity.
	â€¢	Marked as root event, but only a consequence.
	â€¢	Misclassified event type.

â¸»

ğŸ¯ General Guidelines

âœ” Do not assume information beyond the text.
âœ” Accept valid temporal inference when text supports it.
âœ” Focus on narrative value, not ethical weight.
âœ” Be consistent across evaluations.
